
## 下一个 RWKV 模型什么时候发布？

RWKV 没有固定的发布计划，也不承诺何时发布下一个模型。一般来说，负责 RWKV 项目的 BlinkDL 会在模型准备好的第一时间发布新模型。

所以下一个版本的发布时间可能是接下来的几天，或者接下来的几个月。

此外，作为一个开源软件（OSS）模型，我们的训练过程严重依赖于赞助商提供的 GPU 可用性。

通常情况下，当我们发布最新版本的模型时，下一个版本模型的训练/准备过程已经在进行中。

## RWKV 训练数据集是什么？

- RWKV 基础模型主要在 ["the pile"](https://pile.eleuther.ai/) 上训练，附加了其他语言的数据集。
- RWKV raven 是在 gpt4all 的过滤版本上进行指令训练的。
- RWKV World 模型同时在 "the pile" 和 [redpajamas](https://github.com/togethercomputer/RedPajama-Data) 上训练。

## RWKV 模型是否被审查？

RWKV 模型通常不会被审查。然而，尽管我们从 gpt4all 训练数据集中移除了常见的问题（即 “作为 LLM，我无法回答这个问题”），但这并不意味着数据集是完美的。因此，在某些情况下，模型仍然可能自我审查。

## 为什么各种 RWKV 模型在名称中列出了上下文长度（4k / 8k）？

虽然 RWKV 在技术层面上具有 “无限”的上下文长度，但它需要一定上下文长度的训练数据，才能有效执行任务。列出的模型 “上下文长度” 是模型已经训练过的 “有效上下文长度”。对任何超过这个长度的内容，模型的性能预计会大幅下降，因为它从未被训练来处理如此长的上下文。

如果你有训练数据，是可以训练/微调模型到更长的上下文长度的。

## RWKV 是根据什么许可证发布的？

RWKV 及其模型是根据 Apache 2.0 许可证发布的，这意味着它适用于商业和非商业用途。

## 什么是 racoon？

Racoon 是一个社区微调模型，用于聊天和通用用途。由 @nath 创建，你可以在这里找到它：https://huggingface.co/m8than/rwkv-v4-raccoon 。

它有自己独特的个性，如果你需要，它更愿意侮辱用户。

## RWKV 聊天命令是什么？

RWKV.cpp 和 RWKV Discord 聊天机器人包括以下特殊命令，你可以相应地使用它们。

你可以随时配置以下设置：

- `-temp=X`：将模型的温度设置为 X，其中 X 在 0.2 到 5 之间
- `-top_p=Y`：将 top_p 设置在 0.0 到 1.0 之间

你只能在每个提示中使用以下一个命令：

- `+reset`：将当前聊天历史重置为其初始状态
- `+gen`：从提示中继续生成响应
    - `+++`：继续生成响应
    - `++`：尝试一个替代响应
- `+i`：使用提示作为指令生成响应（使用指令模板）
- `+qa`：使用提示作为问题从空白状态生成响应（使用问题 - 答案模板）
- `+`：尝试一个替代聊天回复（默认模式是聊天模式，除非你使用了 +gen / +i / +qa 命令）

这些命令是由 `chat.py` 代码实现的，并不是 RWKV 模型本身的一部分。

## 我想更多地了解架构（隐藏状态、时间混合等），应该去哪里？

参见 [架构页面](../advance/architecture.md)。

## 从头开始训练超过 20B 的 RWKV 需要多少成本？能不能给我一个简单的答案，需要多少钱？

> 简而言之：如果你没有接近 100 万美元的 GPU 时间，不要考虑这个任务。

虽然训练模型的价格不断下降，但大多数人低估了从头开始训练模型的任务成本。

训练模型涉及许多因素，最重要的是数据集大小（即令牌数量）、模型大小和你的时间线。你还需要为训练过程中可能发生的错误买单，以及设置和准备整个训练过程所涉及的人力成本预算。

上述因素使得整个训练过程难以准确预测。（更不用说涉及到那么多钱，你可能会对训练模型使用的数据集有要求，所有额外的数据集需求又需要更多时间和劳动力来进行准备。）

例如，预计 [从头开始训练 LLaMA270B 基础模型](https://twitter.com/moinnadeem/status/1681393075367841792) 需要 260 万美元，使用 2 万亿令牌，而这些成本仅计算了 GPU 时间的花费。

因此，作为基本原则，除非你有接近 100 万美元的 GPU 时间和足够的预算来准备数据集，否则你不应该考虑从头开始训练任何超过 14B 的东西。

虽然理论上 RWKV 作为 RNN，训练起来应该比 transformer 更便宜。但即使是 260 万美元的五分之一，也是大多数个人或公司无法承受的成本。

此时，有些人可能会问：是否可能只在一台机器上训练？而不是昂贵的 GPU 集群？

理论上，只要你有 [模型大小所需的最小 vram](https://wiki.rwkv.com/advance/finetune.html#how-much-gpu-vram-do-you-need)，你就可以在一台机器上训练。

然而，对于足够大的数据集，或在 70B LLaMA2 2 万亿令牌的情况下 ，单个 A100 总共需要 1,720,320 小时，即 196 年。

没有人想等待 190 多年才能完成模型训练，因此我们通常在多个训练节点之间分担工作负载。很不幸，这不是一个完美的可扩展过程。因为我们添加的每一个节点，都会增加效率的惩罚，因为涉及到高通信开销。

最终结果，变成了一个非常复杂的数学问题：“你想要模型多快”与“你能支付多少钱”之间的对比，更快的训练时间通常意味着总体上增加的成本。

这使得根据你想要的模型速度快还是慢，预估的 500 万到 100 万美元都是非常可能的数字。

> 如果你有 GPU 算力可以捐赠给 RWKV ，用于训练开源的软件模型，请通过你的研究机构等渠道请与我们联系 😉（不需要是 100 万美元的量级，即使是小量捐赠也能大有帮助）

## RWKV 支持 “训练并行化” 吗？为什么 RetNet 论文声称不支持？

RWKV 通过 deepspeed 支持跨多个 GPU 的 “训练并行化”。在许多情况下，在类似参数计数的训练速度上超过了 Transformer。

这与 [huggingface](https://huggingface.co/docs/transformers/v4.15.0/parallelism) 或其他 [论文](https://www.researchgate.net/figure/Different-Training-Parallelization-Strategies_fig2_334821612) 采用的定义一致。

RetNet 将 “训练并行化” 定义为在不等待前一个令牌训练完成的情况下对后一个令牌进行损失训练的能力，RWKV 在这个定义上失败了。这已经被 [论文作者](https://web.archive.org/web/20230916013316/https://github.com/microsoft/unilm/issues/1243)确认，他们分别承认 RWKV 在跨多个 GPU 的高吞吐量方面没有问题（根据他们的测试）。

RWKV 不否认在这种另类定义上所做的声明的有效性，例如，在级联到任何其他令牌或层之前，首先需要评估第 1 层令牌。

我们已要求他们更改，因为论文的定义不清楚，可能会引起误解，但我们无法强制对我们无法控制的其他论文出版物进行更改。